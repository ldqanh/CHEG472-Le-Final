# -*- coding: utf-8 -*-
"""Final Exam Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1phfsb5fk1v8J-J39tVJBMX-lt3n2lHR7
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import shap  # For Partial Dependence Plot

#upload the file
uploaded = files.upload()

#load the dataset
file_name = "dataset 4 HTL.xlsx"
df = pd.read_excel(file_name, header = 1)

#display the first few rows
df.head()

# check the data types to identify numeric columns
df.dtypes

df['Biocrude hydrogen content (wt%)'] = pd.to_numeric(df['Biocrude hydrogen content (wt%)'], errors='coerce')
df['Temprature (°C)'] = df['Temprature (°C)'].astype(float)

# check data types again
df.dtypes

# check for missing data
missing_data = df.isnull().sum()

# display columns with missing data
missing_data[missing_data > 0]

# impute missing data with the mean
df['Aqueous phase yield (%)'].fillna(df['Aqueous phase yield (%)'].mean(), inplace=True)
df['Syngas yield (%)'].fillna(df['Syngas yield (%)'].mean(), inplace=True)
df['Hydrochar yield (%)'].fillna(df['Hydrochar yield (%)'].mean(), inplace=True)
df['Biocrude carbon content (wt%)'].fillna(df['Biocrude carbon content (wt%)'].mean(), inplace=True)
df['Biocrude hydrogen content (wt%)'].fillna(df['Biocrude hydrogen content (wt%)'].mean(), inplace=True)
df['Biocrude nitrogen content (wt%)'].fillna(df['Biocrude nitrogen content (wt%)'].mean(), inplace=True)
df['Biocrude oxygen content (wt%)'].fillna(df['Biocrude oxygen content (wt%)'].mean(), inplace=True)
df['Biocrude sulfur content (wt%)'].fillna(df['Biocrude sulfur content (wt%)'].mean(), inplace=True)
df['Biocrude calorific value (MJ/kg)'].fillna(df['Biocrude calorific value (MJ/kg)'].mean(), inplace=True)

# check for missing data
missing_data = df.isnull().sum()

# display columns with missing data
missing_data[missing_data > 0]

# Identify numeric columns
numeric_columns = df.select_dtypes(include=['float']).columns

# Identify categorical columns (assuming they are of type 'object')
categorical_columns = df.select_dtypes(include=['object']).columns

# apply one hot encoded
df_encoded = pd.get_dummies(df, columns=categorical_columns)

df_encoded.head()

# Find duplicate columns
def get_duplicate_columns(df_encoded):
    duplicate_column_names = set()
    for i in range(df_encoded.shape[1]):
        col = df_encoded.iloc[:, i]
        for j in range(i + 1, df_encoded.shape[1]):
            other_col = df_encoded.iloc[:, j]
            if col.equals(other_col):
                duplicate_column_names.add(df_encoded.columns[j])
    return list(duplicate_column_names)
duplicate_columns = get_duplicate_columns(df_encoded) # Pass the DataFrame to the function
print("Duplicate columns:", duplicate_columns)

# Handling the Outliers
# Create the box plot
df.boxplot()
plt.title('Box Plots for Numerical Columns')
plt.xticks(rotation=45)
plt.show()

#Contains ouliers

# Handle outliers
# Calculate the IQR for each numerical column
outlier_threshold = 0.5
q1 = df.select_dtypes(include=np.number).quantile(0.25) # Select numerical columns only
q3 = df.select_dtypes(include=np.number).quantile(0.75) # Select numerical columns only
iqr = q3 - q1
lower_bound = q1 - outlier_threshold * iqr
upper_bound = q3 + outlier_threshold * iqr

# Filter out outliers
df_without_outliers = df[(df[df.select_dtypes(include=np.number).columns] >= lower_bound) & (df[df.select_dtypes(include=np.number).columns] <= upper_bound)]

# Check for the data without outliers
df_without_outliers.boxplot()
plt.title('Box Plots for Numerical Columns (without outliers)')
plt.xticks(rotation=45)
plt.show()

# Outliers are handled by calculating IQR and then removing using lower and upper bound

# Compute correlation matrix for df_without_outliers
corr_matrix_outliers = df_without_outliers.corr()
print(corr_matrix_outliers)

# Correlation Heatmap
plt.figure(figsize=(10, 8))
# Calculate correlation only for numeric columns
numeric_df = df.select_dtypes(include=np.number)  # Select only numeric columns
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

# Plot correlation matrix heatmap for df_without_outliers
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix_outliers, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap for df_without_outliers')
plt.show()

# Machine Learning
# Define the columns to drop as a list
columns_to_drop = ['Biocrude oil yield (%)', 'Aqueous phase yield (%)', 'Syngas yield (%)',
                   'Hydrochar yield (%)', 'Biocrude carbon content (wt%)',
                   'Biocrude hydrogen content (wt%)', 'Biocrude nitrogen content (wt%)',
                   'Biocrude oxygen content (wt%)', 'Biocrude sulfur content (wt%)',
                   'Biocrude calorific value (MJ/kg)']

# Drop the columns using the list
X = df_without_outliers.drop(columns=columns_to_drop, axis=1)

# Define the target columns as a list
target_columns = ['Biocrude oil yield (%)', 'Aqueous phase yield (%)', 'Syngas yield (%)',
                   'Hydrochar yield (%)', 'Biocrude carbon content (wt%)',
                   'Biocrude hydrogen content (wt%)', 'Biocrude nitrogen content (wt%)',
                   'Biocrude oxygen content (wt%)', 'Biocrude sulfur content (wt%)',
                   'Biocrude calorific value (MJ/kg)']

# Select the target columns using the list
y = df_without_outliers[target_columns]

# Continue with train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Models and metrics
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(random_state=42),
    'Support Vector Regressor': SVR(),
    'Decision Tree': DecisionTreeRegressor(random_state=42)
}

def evaluate_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    results = {
        "Train R²": r2_score(y_train, y_train_pred),
        "Test R²": r2_score(y_test, y_test_pred),
        "Train RMSE": np.sqrt(mean_squared_error(y_train, y_train_pred)),
        "Test RMSE": np.sqrt(mean_squared_error(y_test, y_test_pred)),
        "Train MAE": mean_absolute_error(y_train, y_train_pred),
        "Test MAE": mean_absolute_error(y_test, y_test_pred)
    }
    return results

# Impute missing values in X_train and X_test (using SimpleImputer)
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')  # You can choose other strategies like 'median'

# Fit on the training data and transform both training and testing data for X
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Now, impute missing values in y_train and y_test as well
imputer_y = SimpleImputer(strategy='mean')  # Create a separate imputer for y
y_train = imputer_y.fit_transform(y_train)  # Fit and transform y_train
y_test = imputer_y.transform(y_test)       # Transform y_test using the fitted imputer

# Impute missing values in X_train and X_test (using SimpleImputer)
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')  # You can choose other strategies like 'median'

# Fit on the training data and transform both training and testing data for X
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Now, impute missing values in y_train and y_test as well
imputer_y = SimpleImputer(strategy='mean')  # Create a separate imputer for y

# Train separate models for each target column
for i, target_column in enumerate(target_columns): # Get the index (i) and column name
    print(f"Training models for target: {target_column}")

    # Access the target column using its index
    y_train_single = imputer_y.fit_transform(y_train[:, i].reshape(-1, 1)).ravel()
    y_test_single = imputer_y.transform(y_test[:, i].reshape(-1, 1)).ravel()

    for name, model in models.items():
        print(f"Evaluating {name}...")
        results = evaluate_model(model, X_train, y_train_single, X_test, y_test_single)
        print(pd.DataFrame([results]))

from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import PartialDependenceDisplay # Import PartialDependenceDisplay
import matplotlib.pyplot as plt

# Train a model (using Random Forest as an example)
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Select the feature to analyze (replace 'feature_name' with your column name)
feature_name = 'Biomass type'  # Replace with the feature column
# feature_index = X.columns.get_loc(feature_name)  # Get index of the feature - No longer needed

# Generate Partial Dependence Plot using PartialDependenceDisplay
display = PartialDependenceDisplay.from_estimator(model, X_train, [feature_name], # Use feature_name directly
                                                 feature_names=X.columns, grid_resolution=100)

# Show the plot
plt.show()

# Models and metrics
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(random_state=42),
    'Support Vector Regressor': SVR(),
    'Decision Tree': DecisionTreeRegressor(random_state=42)
}

# Train a model (using Random Forest as an example)
model = LinearRegression()
model.fit(X_train, y_train)

# Select the feature to analyze (replace 'feature_name' with your column name)
feature_name = 'Biomass type'  # Replace with the feature column

# Specify the target variable index (0-based)
target_index = 0  # For example, 0 for 'Biocrude oil yield (%)'

# Generate Partial Dependence Plot using PartialDependenceDisplay
display = PartialDependenceDisplay.from_estimator(
    model, X_train, [feature_name],
    feature_names=X.columns, grid_resolution=100,
    target=target_index  # Add the target parameter
)

# Show the plot
plt.show()